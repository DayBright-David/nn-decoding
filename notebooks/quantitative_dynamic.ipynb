{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from functools import partial\n",
    "import itertools\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import statsmodels.formula.api as smf\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\", context=\"paper\", font_scale=3.5, rc={\"lines.linewidth\": 2.5})\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('png')\n",
    "#set_matplotlib_formats('svg')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"../output\")\n",
    "decoder_path = output_path / \"decoders\"\n",
    "bert_encoding_path = output_path / \"encodings\"\n",
    "model_path = output_path / \"bert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = [util.get_encoding_ckpt_id(dir_entry) for dir_entry in bert_encoding_path.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model for model, _, _ in checkpoints]\n",
    "standard_models = [model for model in models if not model.startswith(\"LM_\")]\n",
    "custom_models = [model for model in models if model.startswith(\"LM_\")]\n",
    "\n",
    "runs = sorted(set(run for _, run, _ in checkpoints))\n",
    "checkpoint_steps = sorted(set(step for _, _, step in checkpoints))\n",
    "\n",
    "# Models which should appear in the final report figures\n",
    "report_models = [\"SQuAD\", \"QQP\", \"MNLI\", \"SST\", \"LM\", \"LM_scrambled\", \"LM_scrambled_para\", \"LM_pos\", \"glove\"]\n",
    "\n",
    "# Model subsets to render in different report figures\n",
    "report_model_sets = [\n",
    "    (\"all\", set(report_models)),\n",
    "    (\"standard\", set(report_models) & set(standard_models)),\n",
    "    (\"custom\", set(report_models) & set(custom_models)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_FINAL = True\n",
    "figure_path = Path(\"../reports/figures\")\n",
    "figure_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "report_hues = dict(zip(sorted(report_models), sns.color_palette()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decoder performance data.\n",
    "decoding_perfs = util.load_decoding_perfs(decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save perf data.\n",
    "decoding_perfs.to_csv(output_path / \"decoder_perfs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base decoder data.\n",
    "# TODO refit to current setup -- need to add this decoder learning to pipeline ..\n",
    "base_perfs = {}\n",
    "base_perf_paths = list(Path(\"../models/decoders\").glob(\"encodings-CLS.%s*.csv\" % bert_base_model))\n",
    "for base_decoder_perf_path in tqdm_notebook(base_perf_paths):\n",
    "    subject, = re.findall(r\"-([\\w\\d]+)\\.csv$\", base_decoder_perf_path.name)\n",
    "    perf = pd.read_csv(base_decoder_perf_path,\n",
    "                       usecols=[\"mse\", \"r2\", \"rank_median\", \"rank_mean\", \"rank_min\", \"rank_max\"])\n",
    "    base_perfs[\"_\", 1, 0, subject] = perf\n",
    "    \n",
    "if len(base_perfs) == 0:\n",
    "    raise RuntimeError(\"No base model performance found. Stop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if base_perfs:\n",
    "    decoding_perfs = \\\n",
    "        pd.concat([decoding_perfs,\n",
    "                   pd.concat(base_perfs, names=[\"model\", \"run\", \"step\", \"subject\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load comparison model data.\n",
    "# for other_model in other_models:\n",
    "#     other_perf_paths = list(Path(\"../models/decoders\").glob(\"encodings.%s-*.csv\" % other_model))\n",
    "#     for other_perf_path in tqdm_notebook(other_perf_paths, desc=other_model):\n",
    "#         subject, = re.findall(r\"-([\\w\\d]+)\\.csv$\", other_perf_path.name)\n",
    "#         perf = pd.read_csv(other_perf_path,\n",
    "#                            usecols=[\"mse\", \"r2\", \"rank_median\", \"rank_mean\", \"rank_min\", \"rank_max\"])\n",
    "#         decoding_perfs.loc[other_model, 1, 250, subject] = perf.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each model, load checkpoint data: global step, gradient norm information\n",
    "model_metadata = {}\n",
    "for model, run, step in tqdm_notebook(checkpoints):    \n",
    "    run_dir = model_path / (\"%s-%i\" % (model, run))\n",
    "    \n",
    "    # Fetch corresponding fine-tuning metadata.\n",
    "    ckpt_path = run_dir / (\"model.ckpt-step%i\" % step)\n",
    "\n",
    "    try:\n",
    "        metadata = util.load_bert_finetune_metadata(run_dir, step)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    else:\n",
    "        if metadata[\"steps\"]:\n",
    "            model_metadata[model, run] = pd.DataFrame.from_dict(metadata[\"steps\"], orient=\"index\")\n",
    "            \n",
    "        # SQuAD eval results need to be loaded separately, since they run offline.\n",
    "        if model == \"SQuAD\":\n",
    "            pred_dir = output_path / \"eval_squad\" / (\"SQuAD-%i-%i\" % (run, step))\n",
    "            try:\n",
    "                with (pred_dir / \"results.json\").open(\"r\") as results_f:\n",
    "                    results = json.load(results_f)\n",
    "                    model_metadata[model, run].loc[step][\"eval_accuracy\"] = results[\"best_f1\"] / 100.\n",
    "            except:\n",
    "                print(\"Failed to retrieve eval data for SQuAD-%i-%i\" % (run, step))\n",
    "\n",
    "model_metadata = pd.concat(model_metadata, names=[\"model\", \"run\", \"step\"], sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join decoding data, post-hoc rank evaluation data, and model training metadata into a single df.\n",
    "old_index = decoding_perfs.index\n",
    "df = decoding_perfs.reset_index().join(model_metadata, on=[\"model\", \"run\", \"step\"]).set_index(old_index.names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjects = df.index.get_level_values(\"subject\").unique()\n",
    "all_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    subjects_with_baseline = set(decoding_perfs.loc[\"_\", :, :].index.get_level_values(\"subject\"))\n",
    "except:\n",
    "    subjects_with_baseline = set()\n",
    "    \n",
    "if not subjects_with_baseline == set(all_subjects):        \n",
    "    raise ValueError(\"Cannot proceed. Missing base decoder evaluation for subjects: \" + str(set(all_subjects) - subjects_with_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"eval_accuracy_delta\"] = df.groupby([\"model\", \"run\"]).eval_accuracy.transform(lambda xs: xs - xs.iloc[0])\n",
    "df[\"eval_accuracy_norm\"] = df.groupby([\"model\", \"run\"]).eval_accuracy.transform(lambda accs: (accs - accs.min()) / (accs.max() - accs.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_perf_delta(xs, metric=\"mse\"):\n",
    "    subject = xs.index[0][3]\n",
    "    base_metric = df.loc[\"_\", 1, 0, subject][metric]\n",
    "    return xs - base_metric.item()\n",
    "\n",
    "df[\"decoding_mse_delta\"] = df.groupby([\"model\", \"run\", \"subject\"]).mse.transform(partial(decoding_perf_delta, metric=\"mse\"))\n",
    "df[\"rank_mean_delta\"] = df.groupby([\"model\", \"run\", \"subject\"]).rank_mean.transform(partial(decoding_perf_delta, metric=\"rank_mean\"))\n",
    "df[\"rank_median_delta\"] = df.groupby([\"model\", \"run\", \"subject\"]).rank_median.transform(partial(decoding_perf_delta, metric=\"rank_median\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BINS = 50\n",
    "def bin(xs):\n",
    "    if xs.isnull().values.any(): return np.nan\n",
    "    return pd.cut(xs, np.linspace(xs.min(), xs.max() + 1e-5, NUM_BINS), labels=False)\n",
    "df[\"eval_accuracy_bin\"] = df.groupby([\"model\"]).eval_accuracy.transform(bin)\n",
    "df[\"decoding_mse_bin\"] = df.groupby([\"subject\"]).decoding_mse_delta.transform(bin)\n",
    "df[\"total_global_norms_bin\"] = df.groupby([\"model\"]).total_global_norms.transform(bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLLING_WINDOW_SIZE = 5\n",
    "grouped = df.groupby([\"model\", \"run\", \"subject\"])\n",
    "for col in [\"mse\", \"decoding_mse_delta\", \"eval_accuracy\", \"train_loss\", \"rank_mean\", \"rank_mean_delta\"]:\n",
    "    df[\"%s_rolling\" % col] = grouped[col].transform(lambda rows: rows.rolling(ROLLING_WINDOW_SIZE, min_periods=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training analysis\n",
    "\n",
    "Let's verify that each model is not overfitting; if it is overfitting, restrict our analysis to just the region before overfitting begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.FacetGrid(df.reset_index().melt(id_vars=[\"model\", \"run\", \"step\"],\n",
    "#                                         value_vars=[\"train_loss_rolling\", \"eval_accuracy_rolling\"]),\n",
    "#                   row=\"variable\", col=\"model\", sharex=True, sharey=False, height=4)\n",
    "# g.map(sns.lineplot, \"step\", \"value\", \"run\", ci=None)\n",
    "# g.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "\n",
    "if RENDER_FINAL:\n",
    "    # models which appear on left edge of subfigs in paper\n",
    "    LEFT_EDGE_MODELS = [\"QQP\", \"LM\"]\n",
    "    \n",
    "    training_fig_path = figure_path / \"training\"\n",
    "    training_fig_path.mkdir(exist_ok=True)\n",
    "    shared_kwargs = {\"legend\": False, \"ci\": None}\n",
    "\n",
    "    for model in tqdm_notebook(report_models):\n",
    "        f, (loss_fig, acc_fig) = plt.subplots(2, 1, figsize=(10,15), sharex=True)\n",
    "        try:\n",
    "            local_data = df.loc[model].reset_index()\n",
    "        except KeyError:\n",
    "            print(f\"Missing training data for {model}\")\n",
    "            continue\n",
    "            \n",
    "        ax = sns.lineplot(data=local_data, x=\"step\", y=\"train_loss_rolling\", hue=\"run\", ax=loss_fig, **shared_kwargs)\n",
    "        ax.set_ylabel(\"Training loss\\n(rolling window)\" if model in LEFT_EDGE_MODELS else \"\")\n",
    "        ax.set_xlabel(\"Training step\")\n",
    "        \n",
    "        ax = sns.lineplot(data=local_data, x=\"step\", y=\"eval_accuracy_rolling\", hue=\"run\", ax=acc_fig, **shared_kwargs)\n",
    "        ax.set_ylabel(\"Validation set accuracy\\n(rolling window)\" if model in LEFT_EDGE_MODELS else \"\")\n",
    "        ax.set_xlabel(\"Training step\")\n",
    "        \n",
    "        sns.despine()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(training_fig_path / (\"%s.pdf\" % model))\n",
    "        plt.close()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_DELTA_LABEL = \"$\\Delta$(MSE)\"\n",
    "MAR_DELTA_LABEL = \"$\\Delta$(MAR)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final state analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "\n",
    "if RENDER_FINAL:\n",
    "    final_state_fig_path = figure_path / \"final_state\"\n",
    "    final_state_fig_path.mkdir(exist_ok=True)\n",
    "    metrics = [(\"decoding_mse_delta\", MSE_DELTA_LABEL, None, None),\n",
    "               (\"rank_mean_delta\", MAR_DELTA_LABEL, None, None),\n",
    "               (\"mse\", \"Mean squared error\", 0.00335, 0.00385),\n",
    "               (\"rank_mean\", \"Mean average rank\", 20, 95)]\n",
    "    \n",
    "    for model_set_name, model_set in report_model_sets:\n",
    "        final_df = dfi[(dfi.step == checkpoint_steps[-1]) & (dfi.model.isin(model_set))]\n",
    "        if final_df.empty:\n",
    "            continue\n",
    "\n",
    "        for metric, label, ymin, ymax in tqdm_notebook(metrics, desc=model_set_name):\n",
    "            fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "            # Plot BERT baseline performance.\n",
    "            if \"delta\" not in metric:\n",
    "                # TODO error region instead -- plt.fill_between\n",
    "                ax.axhline(dfi[dfi.model == \"_\"][metric].mean(), linestyle=\"--\", color=\"gray\")\n",
    "\n",
    "            sns.barplot(data=final_df, x=\"model\", y=metric,\n",
    "                        order=final_df.groupby(\"model\")[metric].mean().sort_values().index,\n",
    "                        palette=report_hues, ax=ax)\n",
    "\n",
    "            padding = final_df[metric].var() * 0.005\n",
    "            plt.ylim((ymin or (final_df[metric].min() - padding), ymax or (final_df[metric].max() + padding)))\n",
    "            plt.xlabel(\"Model\")\n",
    "            plt.ylabel(label)\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(final_state_fig_path / (f\"{metric}.{model_set_name}.pdf\"))\n",
    "            #plt.close(fig)\n",
    "        \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "\n",
    "if RENDER_FINAL:\n",
    "    final_state_fig_path = figure_path / \"final_state_within_subject\"\n",
    "    final_state_fig_path.mkdir(exist_ok=True)\n",
    "    metrics = [(\"decoding_mse_delta\", MSE_DELTA_LABEL),\n",
    "               (\"rank_mean_delta\", MAR_DELTA_LABEL),\n",
    "               (\"mse\", \"Mean squared error\"),\n",
    "               (\"rank_mean\", \"Mean average rank\")]\n",
    "    \n",
    "    for model_set_name, model_set in report_model_sets:\n",
    "        final_df = dfi[(dfi.step == checkpoint_steps[-1]) & (dfi.model.isin(model_set))]\n",
    "\n",
    "        for metric, label in tqdm_notebook(metrics, desc=model_set_name):\n",
    "            fig = plt.figure(figsize=(25, 10))\n",
    "            sns.barplot(data=final_df, x=\"model\", y=metric, hue=\"subject\",\n",
    "                        order=final_df.groupby(\"model\")[metric].mean().sort_values().index)\n",
    "            plt.ylabel(label)\n",
    "            plt.xticks(rotation=30, ha=\"right\")\n",
    "            plt.legend(loc=\"center left\", bbox_to_anchor=(1,0.5))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(final_state_fig_path / f\"{metric}.{model_set_name}.pdf\")\n",
    "            plt.close(fig)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "\n",
    "if RENDER_FINAL:\n",
    "    final_state_fig_path = figure_path / \"final_state_within_model\"\n",
    "    final_state_fig_path.mkdir(exist_ok=True)\n",
    "    metrics = [(\"decoding_mse_delta\", MSE_DELTA_LABEL, None, None),\n",
    "               (\"rank_mean_delta\", MAR_DELTA_LABEL, None, None),\n",
    "               (\"mse\", \"Mean squared error\", None, None),\n",
    "               (\"rank_mean\", \"Mean average rank\", None, None)]\n",
    "    \n",
    "    subj_order = dfi[(dfi.step == checkpoint_steps[-1]) & (dfi.model.isin(report_model_sets[0][1]))] \\\n",
    "        .groupby(\"subject\")[metrics[0][0]].mean().sort_values().index\n",
    "    \n",
    "    for model_set_name, model_set in report_model_sets:\n",
    "        final_df = dfi[(dfi.step == checkpoint_steps[-1]) & (dfi.model.isin(model_set))]\n",
    "\n",
    "        for metric, label, ymin, ymax in tqdm_notebook(metrics, desc=model_set_name):\n",
    "            fig = plt.figure(figsize=(25, 10))\n",
    "            sns.barplot(data=final_df, x=\"subject\", y=metric, hue=\"model\",\n",
    "                        order=subj_order)\n",
    "            \n",
    "            padding = final_df[metric].var() * 0.005\n",
    "            plt.ylim((ymin or (final_df[metric].min() - padding), ymax or (final_df[metric].max() + padding)))\n",
    "            plt.xlabel(\"Subject\")\n",
    "            plt.ylabel(label)\n",
    "            \n",
    "            plt.legend(loc=\"center left\", bbox_to_anchor=(1,0.5))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(final_state_fig_path / f\"{metric}.{model_set_name}.pdf\")\n",
    "            plt.close(fig)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# g = sns.FacetGrid(dfi, col=\"run\", size=6)\n",
    "# g.map(sns.lineplot, \"step\", \"decoding_mse_delta\", \"model\").add_legend()\n",
    "\n",
    "# plt.xlabel(\"Fine-tuning step\")\n",
    "# plt.ylabel(MSE_DELTA_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.FacetGrid(dfi, col=\"run\", size=6)\n",
    "# g.map(sns.lineplot, \"step\", \"rank_mean_delta\", \"model\").add_legend()\n",
    "\n",
    "# plt.xlabel(\"Fine-tuning step\")\n",
    "# plt.ylabel(MAR_DELTA_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.lineplot(data=dfi, x=\"step\", y=\"decoding_mse_delta_rolling\", hue=\"model\", ax=ax)\n",
    "\n",
    "plt.xlabel(\"Fine-tuning step\")\n",
    "plt.ylabel(MSE_DELTA_LABEL)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.lineplot(data=dfi, x=\"step\", y=\"rank_mean_delta_rolling\", hue=\"model\", ax=ax)\n",
    "\n",
    "plt.xlabel(\"Fine-tuning step\")\n",
    "plt.ylabel(MAR_DELTA_LABEL)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.lineplot(data=dfi[dfi.model.str.startswith(\"LM\")], x=\"step\", y=\"decoding_mse_delta_rolling\", hue=\"model\", ax=ax)\n",
    "\n",
    "plt.xlabel(\"Fine-tuning step\")\n",
    "plt.ylabel(MSE_DELTA_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.lineplot(data=dfi[dfi.model.str.startswith(\"LM\")], x=\"step\", y=\"rank_mean_delta_rolling\", hue=\"model\", ax=ax)\n",
    "\n",
    "plt.xlabel(\"Fine-tuning step\")\n",
    "plt.ylabel(MAR_DELTA_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib agg\n",
    "\n",
    "if RENDER_FINAL:\n",
    "    trajectory_fig_dir = figure_path / \"trajectories\"\n",
    "    trajectory_fig_dir.mkdir(exist_ok=True)\n",
    "    metrics = [(\"decoding_mse_delta\", MSE_DELTA_LABEL),\n",
    "               (\"rank_mean_delta\", MAR_DELTA_LABEL),\n",
    "               (\"decoding_mse_delta_rolling\", MSE_DELTA_LABEL),\n",
    "               (\"rank_mean_delta_rolling\", MAR_DELTA_LABEL)]\n",
    "\n",
    "    for model_set_name, model_set in report_model_sets:\n",
    "        for metric, label in tqdm_notebook(metrics, desc=model_set_name):\n",
    "            fig = plt.figure(figsize=(18, 10))\n",
    "            sns.lineplot(data=dfi[dfi.model.isin(model_set) & (~dfi.model.isin(other_models))],\n",
    "                         x=\"step\", y=metric, hue=\"model\", palette=report_hues)\n",
    "            plt.xlim((0, checkpoint_steps[-1]))\n",
    "            plt.xlabel(\"Fine-tuning step\")\n",
    "            plt.ylabel(label)\n",
    "            plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(trajectory_fig_dir / f\"{metric}.{model_set_name}.pdf\")\n",
    "            plt.close(fig)\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.FacetGrid(dfi[dfi.model != \"_\"], col=\"model\", row=\"run\", size=6)\n",
    "# g.map(sns.lineplot, \"step\", \"decoding_mse_delta\", \"subject\", ci=None).add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.FacetGrid(dfi, col=\"model\", row=\"run\", size=6)\n",
    "# g.map(sns.lineplot, \"step\", \"rank_median_delta\", \"subject\", ci=None).add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient norm analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(figsize=(10, 8))\n",
    "# sns.lineplot(data=dfi, y=\"decoding_mse_delta\", x=\"total_global_norms_bin\", hue=\"model\", ax=ax)\n",
    "# ax.set_title(\"Decoding performance delta vs. binned total global gradient norm\")\n",
    "# ax.set_xlabel(\"Cumulative global gradient norm bin\")\n",
    "# ax.set_ylabel(MSE_DELTA_LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.FacetGrid(dfi, col=\"model\", row=\"run\", size=6, sharex=False, sharey=True)\n",
    "#g.map(sns.lineplot, \"total_global_norms\", \"decoding_mse_delta\", \"subject\", ci=None).add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval accuracy analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.FacetGrid(dfi, col=\"model\", row=\"run\", sharex=False, sharey=True, size=7)\n",
    "#g.map(sns.lineplot, \"eval_accuracy\", \"decoding_mse_delta\", \"subject\", ci=None).add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per-subject analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(14, 9))\n",
    "dff = pd.DataFrame(dfi[dfi.step == checkpoint_steps[-1]].groupby([\"model\", \"run\"]).apply(lambda xs: xs.groupby(\"subject\").decoding_mse_delta.mean()).stack()).reset_index()\n",
    "sns.barplot(data=dff, x=\"model\", hue=\"subject\", y=0, ax=ax)\n",
    "plt.title(\"subject final decoding mse delta, averaging across runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(14, 9))\n",
    "dff = pd.DataFrame(dfi[dfi.step == checkpoint_steps[-1]].groupby([\"model\", \"run\"]).apply(lambda xs: xs.groupby(\"subject\").rank_mean_delta.mean()).stack()).reset_index()\n",
    "sns.barplot(data=dff, x=\"model\", hue=\"subject\", y=0, ax=ax)\n",
    "plt.title(\"subject final rank mean delta, averaging across runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(14, 9))\n",
    "dff = pd.DataFrame(dfi.groupby([\"model\", \"run\"]).apply(lambda xs: xs.groupby(\"subject\").decoding_mse_delta.max()).stack()).reset_index()\n",
    "sns.violinplot(data=dff, x=\"subject\", y=0)\n",
    "sns.stripplot(data=dff, x=\"subject\", y=0, edgecolor=\"white\", linewidth=1, alpha=0.7, ax=ax)\n",
    "plt.title(\"subject max decoding mse delta, averaging across models and runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(14, 9))\n",
    "dff = pd.DataFrame(dfi.groupby([\"model\", \"run\"]).apply(lambda xs: xs.groupby(\"subject\").decoding_mse_delta.min()).stack()).reset_index()\n",
    "sns.violinplot(data=dff, x=\"subject\", y=0)\n",
    "sns.stripplot(data=dff, x=\"subject\", y=0, edgecolor=\"white\", linewidth=1, alpha=0.7, ax=ax)\n",
    "plt.title(\"subject min decoding mse delta, averaging across models and runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical analyses\n",
    "\n",
    "First, some data prep for comparing final vs. start states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_comp = df.query(\"step == %i\" % checkpoint_steps[-1]).reset_index(level=\"step\", drop=True).sort_index()\n",
    "# Join data from baseline\n",
    "perf_comp = perf_comp.join(df.loc[\"_\", 1, 0].rename(columns=lambda c: \"start_%s\" % c))\n",
    "perf_comp = perf_comp.join(df.loc[\"glove\", 1, 250].rename(columns=lambda c: \"glove_%s\" % c))\n",
    "perf_comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(perf_comp.mse - perf_comp.start_mse).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_compi = perf_comp.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantitative tests:\n",
    " \n",
    "1. for any GLUE task g, MSE(g after 250) > MSE(LM)\n",
    "2. for any LM_scrambled_para task t, MSE(t after 250) < MSE(LM)\n",
    "3. for any GLUE task g, MAR(g after 250) > MAR(LM)\n",
    "4. for any LM_scrambled_para task t, MAR(t after 250) < MAR(LM)\n",
    "5. MSE(LM after 250) =~ MSE(LM)\n",
    "6. MAR(LM after 250) =~ MSE(LM)\n",
    "7. for any LM_scrambled_para task t, MSE(t after 250) < MSE(glove)\n",
    "8. for any LM_scrambled_para task t, MAR(t after 250) < MAR(glove)\n",
    "9. for any LM_pos task t, MSE(t after 250) > MSE(LM)\n",
    "10. for any LM_pos task t, MAR(t after 250) > MAR(LM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = perf_compi[~perf_compi.model.str.startswith((\"_\", \"LM\", \"glove\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.mse.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.start_mse.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_rel(sample.mse, sample.start_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 1 (split across models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for model in standard_models:\n",
    "    if model in [\"LM\", \"glove\"]: continue\n",
    "    sample = perf_compi[perf_compi.model == model]\n",
    "    results.append((model,) + st.ttest_rel(sample.mse, sample.start_mse))\n",
    "    \n",
    "pd.DataFrame(results, columns=[\"model\", \"tval\", \"pval\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = perf_compi[perf_compi.model == \"LM_scrambled_para\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.mse.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.start_mse.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_rel(sample.mse, sample.start_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = perf_compi[~perf_compi.model.str.startswith((\"_\", \"LM\", \"glove\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.rank_mean.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.start_rank_mean.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_rel(sample.rank_mean, sample.start_rank_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 3 (split across models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for model in standard_models:\n",
    "    if model in [\"LM\", \"glove\"]: continue\n",
    "    sample = perf_compi[perf_compi.model == model]\n",
    "    results.append((model,) + st.ttest_rel(sample.rank_mean, sample.start_rank_mean))\n",
    "    \n",
    "pd.DataFrame(results, columns=[\"model\", \"tval\", \"pval\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = perf_compi[perf_compi.model == \"LM_scrambled_para\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.rank_mean.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.start_rank_mean.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_rel(sample.rank_mean, sample.start_rank_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = perf_compi[perf_compi.model == \"LM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.mse.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.start_mse.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_rel(sample.mse, sample.start_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = perf_compi[perf_compi.model == \"LM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.rank_mean.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.start_rank_mean.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_rel(sample.rank_mean, sample.start_rank_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = perf_compi[perf_compi.model == \"LM_scrambled_para\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.mse.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.glove_mse.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_rel(sample.mse, sample.glove_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = perf_compi[perf_compi.model == \"LM_scrambled_para\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.rank_mean.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.glove_rank_mean.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_rel(sample.rank_mean, sample.glove_rank_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = perf_compi[perf_compi.model == \"LM_pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.mse.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.start_mse.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_rel(sample.mse, sample.start_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(20,20))\n",
    "sns.barplot(data=pd.melt(sample, id_vars=[\"subject\"], value_vars=[\"mse\", \"start_mse\"]),\n",
    "            x=\"subject\", y=\"value\", hue=\"variable\")\n",
    "plt.ylim((0.0033, 0.0038))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = perf_compi[perf_compi.model == \"LM_pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.rank_mean.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.start_rank_mean.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.ttest_rel(sample.rank_mean, sample.start_rank_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
