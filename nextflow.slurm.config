/**
 * This configuration file specifies an example deployment of the pipeline for
 * a SLURM HPC cluster. This example can be repurposed to fit other HPC setups,
 * from PBS to Kubernetes. See the Nextflow docs for more information:
 * https://www.nextflow.io/docs/latest/executor.html
 */
process {
    /* Specify an HPC queue. */
    /* queue = "cpl" */

    /**
     * Pipeline processes are assigned "labels" according to their
     * computational requirements. Here we can specify the actual effects of
     * each label when the pipeline runs on your system.
     *
     * This first label, `small`, describes simple tasks which can easily run
     * on a single CPU with minimal RAM -- e.g. downloading a file.
     */
    withLabel: 'small' {
        executor = 'local'
        time = '1h'
    }

    /**
     * `medium` describes tasks which require moderate host memory, e.g.
     * loading brain images and learning linear regression models.
     */
    withLabel: 'medium' {
        executor = 'slurm'

        time = '1d'
        memory = '8G'
    }

    /**
     * `gpu_medium` describes tasks which require a GPU with moderate memory
     * and moderate host RAM, for e.g. holding a dataset in memory and running
     * neural network feed-forward inference.
     */
    withLabel: 'gpu_medium' {
        executor = 'slurm'
        containerOptions = "--nv"
        clusterOptions = "--gres=gpu:tesla-k80:1"

        time = '1h'
        memory = '8G'
    }

    /**
     * `gpu_large` describes tasks which require a GPU with lots of memory and
     * large host RAM, for e.g. holding a training dataset in memory and
     * running neural network training.
     */
    withLabel: 'gpu_large' {
        executor = 'slurm'
        containerOptions = "--nv"
        clusterOptions = '--gres=gpu:GEFORCEGTX1080TI:1'

        time = '1d'
        memory = '8G'
    }
}

executor {
    $slurm {
        // Limit number of parallel SLURM jobs to 16.
        queueSize = 16
    }
}

// There should be no need to edit below this line.
//////////////////////////////////////////

params.bert_container = "library://jon/default/bert:base-gpu"
params.structural_probes_container = "library://jon/default/structural-probes:latest"
params.decoding_container = "library://jon/default/nn-decoding:emnlp2019"

singularity {
    enabled = true
    envWhitelist = "CUDA_VISIBLE_DEVICES"
    autoMounts = true
}
report.enabled = true

