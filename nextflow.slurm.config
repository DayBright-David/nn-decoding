
process {
    /* queue = "cpl" */

    /**
     * Pipeline processes are assigned "labels" according to their
     * computational requirements. Here we can specify the actual effects of
     * each label when the pipeline runs on your system.
     *
     * This first label, `small`, describes simple tasks which can easily run
     * on a single CPU with minimal RAM -- e.g. downloading a file.
     */
    withLabel: 'small' {
        executor = 'local'
        time = '1h'
    }

    /**
     * `medium` describes tasks which require moderate host memory, e.g.
     * loading brain images and learning linear regression models.
     */
    withLabel: 'medium' {
        executor = 'slurm'

        time = '1d'
        memory = '8G'
    }

    /**
     * `gpu_medium` describes tasks which require a GPU with moderate memory
     * and moderate host RAM, for e.g. holding a dataset in memory and running
     * neural network feed-forward inference.
     */
    withLabel: 'gpu_medium' {
        executor = 'slurm'
        containerOptions = "--nv"
        clusterOptions = "--gres=gpu:tesla-k80:1"

        time = '1h'
        memory = '8G'
    }

    /**
     * `gpu_large` describes tasks which require a GPU with lots of memory and
     * large host RAM, for e.g. holding a training dataset in memory and
     * running neural network training.
     */
    withLabel: 'gpu_large' {
        executor = 'slurm'
        containerOptions = "--nv"
        clusterOptions = '--gres=gpu:GEFORCEGTX1080TI:1'

        time = '1d'
        memory = '8G'
    }
}

executor {
    $slurm {
        // Limit number of parallel SLURM jobs to 16.
        queueSize = 16
    }
}

// There should be no need to edit below this line.
//////////////////////////////////////////

singularity {
    enabled = true
    envWhitelist = "CUDA_VISIBLE_DEVICES"
    autoMounts = true
}
report.enabled = true

